# ğŸ­ Projeto Data Lake Local com PostgreSQL + Apache Airflow

Este projeto implementa um **mini Data Lake local** rodando em um servidor Linux, com banco de dados **PostgreSQL** e orquestraÃ§Ã£o de processos via **Apache Airflow**.  
O objetivo Ã© criar uma infraestrutura robusta e escalÃ¡vel para integrar e armazenar dados industriais, aplicando boas prÃ¡ticas de engenharia de dados

---

## ğŸ§  MotivaÃ§Ã£o

Este projeto foi criado como parte da minha jornada de transiÃ§Ã£o da Ã¡rea de automaÃ§Ã£o industrial para engenharia de dados.
Com mais de 15 anos de experiÃªncia em chÃ£o de fÃ¡brica e sistemas industriais (especialmente Siemens PLCs), 
minha missÃ£o Ã© integrar esse conhecimento com pipelines de dados, ETL, e Machine Learning, comeÃ§ando por uma base sÃ³lida de ingestÃ£o e orquestraÃ§Ã£o.

---

## âš™ï¸ Stack Utilizada

| Camada               | Ferramenta        | FunÃ§Ã£o                                                                 |
|----------------------|-------------------|------------------------------------------------------------------------|
| SO                   | Linux (Ubuntu)    | Ambiente principal                                                     |
| Banco de Dados       | PostgreSQL        | Armazenamento dos dados particionados por data                        |
| OrquestraÃ§Ã£o         | Apache Airflow    | Agendamento e execuÃ§Ã£o das pipelines                                  |
| AutomaÃ§Ã£o de Backup  | crontab           | Backups automÃ¡ticos do banco de dados                                  |
| Scripts ETL          | Python            | IngestÃ£o e transformaÃ§Ã£o de dados brutos (TXT, CSV, sensores, etc.)   |

---

## ğŸ§± Estrutura do Projeto

datalake_local/
â”œâ”€â”€ dags/
â”‚ â”œâ”€â”€ criar_particoes_diarias.py
â”‚ â””â”€â”€ etl_maquina1.py
â”œâ”€â”€ etl/
â”‚ â”œâ”€â”€ processa_maquina1.py
â”œâ”€â”€ sql/
â”‚ â”œâ”€â”€ criar_tabelas.sql
â”‚ â””â”€â”€ criar_particao.sql
â”œâ”€â”€ backups/
â”‚ â””â”€â”€ (arquivos de backup .sql diÃ¡rios)
â”œâ”€â”€ logs/
â”‚ â””â”€â”€ (logs de execuÃ§Ã£o)
â”œâ”€â”€ crontab/
â”‚ â””â”€â”€ backup_postgres.sh

## ğŸ”„ Funcionamento

### 1. AutomaÃ§Ãµes
- criados serviÃ§os usando o systemd para rodar o aiflow webserver e scheduler automaticamente ao inciar a maquina

### 2. ETL das MÃ¡quinas

- A **MÃ¡quina 1** gera arquivos logs `.txt` . (rodando windows xp) - 
- arquivos sao copiados para pasta local (temp) usando smbclient - DAG airflow
- uma outra dag usando rsync monitora a pasta temp como sensor de alteraÃ§Ã£o de dados
- Scripts em Python leem os arquivos, tratam os dados e inserem no PostgreSQL

### 3. Particionamento

- Tabelas sÃ£o **particionadas por data**
- Uma DAG no Airflow roda todo dia Ã  00:00 para criar a partiÃ§Ã£o do dia atual

### 4. Backup DiÃ¡rio

- Script no `crontab` gera backup `.sql` do banco todos os dias Ã  01:00
- Os arquivos de backup sÃ£o salvos na partiÃ§Ã£o dedicada do servidor

### 5. OrquestraÃ§Ã£o com Airflow

- O Airflow roda os servicos scheduler e webserver 
- Logs sÃ£o registrados localmente para anÃ¡lise de falhas
- Monitoramento inicial feito manualmente, com planos futuros para uso de **Prometheus + Grafana**

---

## ğŸ› ï¸ Como Rodar

### Requisitos

- Ubuntu 20.04+
- Python 3.10+
- PostgreSQL 14+
- Apache Airflow 2.9 (instalado via `pip`)
- PermissÃ£o para editar crontab
